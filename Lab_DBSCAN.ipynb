{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TadGreen/datamining/blob/main/Lab_DBSCAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2UDvX2B75gy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate a 2D dataset with non-convex clusters\n"
      ],
      "metadata": {
        "id": "XGXu6rzc8BRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1])\n",
        "plt.title(\"Dataset for DBSCAN\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6DjrE_Pt8Bqx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Applying DBSCAN"
      ],
      "metadata": {
        "id": "yKo_xBHE8S7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan_0_1 = DBSCAN(eps=0.1, min_samples=5)\n",
        "labels_0_1 = dbscan_0_1.fit_predict(X_scaled)\n",
        "\n",
        "# Plot clusters\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_0_1, cmap='plasma')\n",
        "plt.title(\"DBSCAN Clustering with eps=0.1\")\n",
        "\n",
        "# Identify noise\n",
        "noise_points_0_1 = np.sum(labels_0_1 == -1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_0_1, cmap='plasma')\n",
        "plt.scatter(X_scaled[labels_0_1 == -1, 0], X_scaled[labels_0_1 == -1, 1], c='red', marker='x', s=100, label='Noise')\n",
        "plt.title(f\"Noise points with eps=0.1: {noise_points_0_1}\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FuncnrYA8T6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Can you change epsilon and min values to observe how things change? For instance,\n",
        "\n",
        "* Try eps=0.1, eps=0.5 and observe the difference.\n",
        "* What happens when you change min_samples to 3 or 10?\n",
        "* Can you also identify noise for each trial. (Hint: np.sum(labels == -1) after you your clusters)"
      ],
      "metadata": {
        "id": "tmZGGT9C8j8M"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eae3b4c2"
      },
      "source": [
        "dbscan_0_5 = DBSCAN(eps=0.5, min_samples=5)\n",
        "labels_0_5 = dbscan_0_5.fit_predict(X_scaled)\n",
        "\n",
        "# Plot clusters\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_0_5, cmap='plasma')\n",
        "plt.title(\"DBSCAN Clustering with eps=0.5\")\n",
        "\n",
        "# Identify noise\n",
        "noise_points_0_5 = np.sum(labels_0_5 == -1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_0_5, cmap='plasma')\n",
        "plt.scatter(X_scaled[labels_0_5 == -1, 0], X_scaled[labels_0_5 == -1, 1], c='red', marker='x', s=100, label='Noise')\n",
        "plt.title(f\"Noise points with eps=0.5: {noise_points_0_5}\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan_min_samples_3 = DBSCAN(eps=0.5, min_samples=3)\n",
        "labels_min_samples_3 = dbscan_min_samples_3.fit_predict(X_scaled)\n",
        "\n",
        "# Plot clusters\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_min_samples_3, cmap='plasma')\n",
        "plt.title(\"DBSCAN Clustering with min_samples=3 (eps=0.5)\")\n",
        "\n",
        "# Identify noise\n",
        "noise_points_min_samples_3 = np.sum(labels_min_samples_3 == -1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_min_samples_3, cmap='plasma')\n",
        "plt.scatter(X_scaled[labels_min_samples_3 == -1, 0], X_scaled[labels_min_samples_3 == -1, 1], c='red', marker='x', s=100, label='Noise')\n",
        "plt.title(f\"Noise points with min_samples=3: {noise_points_min_samples_3}\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L5qm_OoQ88Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z49eNdYb-ZHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Use a real dataset:\n",
        "* Try load_iris() from sklearn.datasets.\n",
        "* Apply DBSCAN"
      ],
      "metadata": {
        "id": "NCghTHIr-aC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "# y_iris = iris.target # Not used for unsupervised clustering\n",
        "\n",
        "# Standardize the data\n",
        "scaler_iris = StandardScaler()\n",
        "X_iris_scaled = scaler_iris.fit_transform(X_iris)\n",
        "\n",
        "# Apply DBSCAN\n",
        "# Using initial parameters; these might need tuning for optimal results\n",
        "dbscan_iris = DBSCAN(eps=0.7, min_samples=5) # Adjusted eps slightly for better initial visual\n",
        "labels_iris = dbscan_iris.fit_predict(X_iris_scaled)\n",
        "\n",
        "# Visualize the clusters (using the first two features for simplicity)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_iris_scaled[:, 0], X_iris_scaled[:, 1], c=labels_iris, cmap='viridis')\n",
        "plt.title(\"DBSCAN Clustering on Iris Dataset (first two features)\")\n",
        "plt.xlabel(iris.feature_names[0])\n",
        "plt.ylabel(iris.feature_names[1])\n",
        "\n",
        "# Identify noise\n",
        "noise_points_iris = np.sum(labels_iris == -1)\n",
        "plt.scatter(X_iris_scaled[labels_iris == -1, 0], X_iris_scaled[labels_iris == -1, 1],\n",
        "            c='red', marker='x', s=100, label='Noise')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Number of clusters found: {len(set(labels_iris)) - (1 if -1 in labels_iris else 0)}\")\n",
        "print(f\"Number of noise points: {noise_points_iris}\")"
      ],
      "metadata": {
        "id": "3VshLIzs-T82"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}